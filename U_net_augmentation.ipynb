{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ChL1PDfpxd0s",
        "cu3vgZW3FW4g",
        "xXGx04FKFJBI",
        "pNhqELxQEE37",
        "s5BxQcqlFPqf",
        "B9IRxAPKD21Q",
        "ra6jlAlntX0M"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niccolo-Rocchi/Medical_Imaging_project/blob/main/U_net_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation on Pneuomothorax dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Task: Binary Mask Prediction (Pneuomothorax Area) \n",
        "\n",
        "Pneuomothorax Challenge: https://siim.org/page/pneumothorax_challenge\n",
        "\n",
        "*Authors: Pirola Federico, Rocchi Niccolò*\n",
        "\n",
        "*Università degli studi di Milano-Bicocca, Milan, Italy*"
      ],
      "metadata": {
        "id": "_KJ0NO6_wxMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-net segmentation, with data augmentation"
      ],
      "metadata": {
        "id": "o3UDS-OAgPco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "KGjYpHWowyZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pydicom albumentations"
      ],
      "metadata": {
        "id": "L_Suctie9Jug"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For reading files\n",
        "from pydicom import dcmread \n",
        "import glob\n",
        "from google.colab import drive\n",
        "# For dealing with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "random.seed(123)\n",
        "import os\n",
        "# For plots\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imshow\n",
        "# For nets\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers import Concatenate as concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "# Data augmentation\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from albumentations.core.transforms_interface import DualTransform, to_tuple"
      ],
      "metadata": {
        "id": "uDYvI8wlw4Nh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive data\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/pneumotorax_data"
      ],
      "metadata": {
        "id": "75NvFbToVeE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f07748-319a-4801-cf9b-fb03fd4b9607"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/DS Lab in Medicine - projects/Medical Imaging - project/pneumotorax_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data upload"
      ],
      "metadata": {
        "id": "hr1CckcRggwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data upload from `Data_Preprocesing` notebook\n",
        "train_set = pd.read_csv(\"train_set.csv\")\n",
        "val_set = pd.read_csv(\"val_set.csv\")"
      ],
      "metadata": {
        "id": "9ATQc5RBVZfR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator for the net"
      ],
      "metadata": {
        "id": "FtNcUJNjgjuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RLE to mask conversion provided by competition organizers with the dataset.\n",
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width*height)\n",
        "\n",
        "    if rle != ' -1':\n",
        "      array = np.asarray([int(x) for x in rle.split()])\n",
        "      starts = array[0::2]\n",
        "      lengths = array[1::2]\n",
        "\n",
        "      current_position = 0\n",
        "      for index, start in enumerate(starts):\n",
        "          current_position += start\n",
        "          mask[current_position:current_position+lengths[index]] = 1\n",
        "          current_position += lengths[index]\n",
        "\n",
        "    return mask.reshape(width, height, order='F')"
      ],
      "metadata": {
        "id": "R9z7YCstx0wK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation list\n",
        "augment_list = [\n",
        "    A.RandomResizedCrop(1024,1024,scale = (1,1), ratio=(0.8, 1.25),p=1),\n",
        "    A.RandomBrightnessContrast(p=1,brightness_limit=0.4,contrast_limit=0.4), \n",
        "    A.ShiftScaleRotate(p=1,scale_limit=0.3,rotate_limit=30,shift_limit=0.1,border_mode=cv2.BORDER_CONSTANT), \n",
        "    A.GridDistortion(p=1,distort_limit=0.5), \n",
        "    A.ElasticTransform(p=1,border_mode=cv2.BORDER_CONSTANT)\n",
        "]"
      ],
      "metadata": {
        "id": "tP8IFSk7bcnA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input for Keras' fit function\n",
        "class DataGenerator:\n",
        "\n",
        "  # Method that yields (image, mask) tuple\n",
        "  def data_generator(self, data):\n",
        "    while True:\n",
        "      i = 0\n",
        "      while i < len(data):\n",
        "        # Extract ID and its encoded pixels\n",
        "        id, rle = data[[\"ImageId\",\" EncodedPixels\"]].iloc[i]\n",
        "        # Convert encoded pixels to mask\n",
        "        mask = rle2mask(rle, 1024, 1024)\n",
        "        # Read the image associate to ImageId\n",
        "        try:\n",
        "          dcm_file = dcmread(f\"./dicom_files/{id}.dcm\")\n",
        "        except:\n",
        "          continue\n",
        "        dcm_image = dcm_file.pixel_array\n",
        "        # Rescale image\n",
        "        image = dcm_image/255\n",
        "        # Expand dimensions\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        # Resize image and mask\n",
        "        mask = tf.keras.layers.Resizing(256, 256, interpolation=\"nearest\", crop_to_aspect_ratio=False)(mask)\n",
        "        image = tf.keras.layers.Resizing(256, 256, interpolation=\"bilinear\", crop_to_aspect_ratio=False)(image)\n",
        "        # Mask to categorical\n",
        "        mask = tf.keras.utils.to_categorical(mask, num_classes = 2)\n",
        "        yield (image, mask)\n",
        "        i += 1\n",
        "\n",
        "  def data_generator_augment(self, data):\n",
        "    while True:\n",
        "      i = 0\n",
        "      while i < len(data):\n",
        "        # Extract ID and its encoded pixels\n",
        "        id, rle = data[[\"ImageId\",\" EncodedPixels\"]].iloc[int(i)]\n",
        "        # Convert encoded pixels to mask\n",
        "        mask = rle2mask(rle, 1024, 1024)\n",
        "        # Read the image associate to ImageId\n",
        "        try:\n",
        "          dcm_file = dcmread(f\"./dicom_files/{id}.dcm\")\n",
        "        except:\n",
        "          continue\n",
        "        dcm_image = dcm_file.pixel_array\n",
        "        # Rescale image\n",
        "        image = dcm_image/255\n",
        "        if i != int(i):\n",
        "          # Select randomly an augmentation method\n",
        "          augment_method = list(np.random.choice(a = augment_list, replace = False, size = 1))\n",
        "          transform = A.Compose(augment_method)\n",
        "          if augment_method[0] != augment_list[1]:\n",
        "            # Transform image and mask based on the augmentation method\n",
        "            transformed = transform(image=image,mask=mask)\n",
        "            image = transformed['image']\n",
        "            mask = transformed['mask']\n",
        "          else:\n",
        "            image = transform(image=np.float32(image))['image']\n",
        "        # Expand dimensions\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        # Resize image and mask\n",
        "        mask = tf.keras.layers.Resizing(256, 256, interpolation=\"nearest\", crop_to_aspect_ratio=False)(mask)\n",
        "        image = tf.keras.layers.Resizing(256, 256, interpolation=\"bilinear\", crop_to_aspect_ratio=False)(image)\n",
        "        # Mask to categorical\n",
        "        mask = tf.keras.utils.to_categorical(mask, num_classes = 2)\n",
        "        yield (image, mask)\n",
        "        i += 0.5\n",
        "\n",
        "   # Create train generator for net input (augmented images)\n",
        "  def train_generator_augmented(self, batch_size):\n",
        "    # Create a tensorflow iterator\n",
        "    tf_iterator = tf.data.Dataset.from_generator(lambda: self.data_generator_augment(train_set), output_types=(tf.float64, tf.float64))\n",
        "    # Create batch size\n",
        "    tf_iterator = tf_iterator.batch(batch_size)\n",
        "    # Return generator\n",
        "    return tf_iterator\n",
        "\n",
        "  # Create validation generator for net input\n",
        "  def val_generator(self, batch_size):\n",
        "    # Create a tensorflow iterator\n",
        "    tf_iterator = tf.data.Dataset.from_generator(lambda: self.data_generator(val_set), output_types=(tf.float64, tf.float64))\n",
        "    # Create batch size\n",
        "    tf_iterator = tf_iterator.batch(batch_size)\n",
        "    # Return generator\n",
        "    return tf_iterator"
      ],
      "metadata": {
        "id": "i2Torw9AR0Ux"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-net with augmentation"
      ],
      "metadata": {
        "id": "IDaILNrtgpc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "ChL1PDfpxd0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intersection-Over-Union (IoU), also known as the Jaccard Index\n",
        "def IoU(y_true, y_pred):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    return intersection / union\n",
        "\n",
        "# True Positive Rate\n",
        "def true_positive_rate(y_true, y_pred):\n",
        "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
        "\n",
        "# Dice coefficient\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    sum = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "    return K.mean( (2. * intersection + smooth) / (sum + smooth), axis=0)"
      ],
      "metadata": {
        "id": "6OdZ3HuFx7l9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss functions"
      ],
      "metadata": {
        "id": "1O01XXqvARWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FqMreoG0e3UD"
      },
      "outputs": [],
      "source": [
        "# Focal loss\n",
        "def focal_loss(gamma=2., alpha=.25):\n",
        "\tdef focal_loss_fixed(y_true, y_pred):\n",
        "\t\tpt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "\t\tpt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\t\treturn -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1+K.epsilon())) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
        "\treturn focal_loss_fixed\n",
        "\n",
        "# Dice loss\n",
        "def dice_loss(y_true, y_pred, smooth=1):\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "# BinaryCrossEntropy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Net definition and train"
      ],
      "metadata": {
        "id": "banPi8Wuxfyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "metrics = [dice_coef, true_positive_rate, IoU]\n",
        "\n",
        "# Loss\n",
        "loss = dice_loss\n",
        "\n",
        "# Batch and epochs\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "assert(batch_size <= 32)\n",
        "\n",
        "# Callbacks\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "sJtc0z0JWbbJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Net\n",
        "def create_model(loss):\n",
        "\n",
        "  # Model description\n",
        "  inputs = Input((256, 256, 1))\n",
        "\n",
        "  c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n",
        "  c1 = Dropout(0.1) (c1)\n",
        "  c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "  p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "  c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "  c2 = Dropout(0.1) (c2)\n",
        "  c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "  p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "  c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "  c3 = Dropout(0.2) (c3)\n",
        "  c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "  p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "  c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "  c4 = Dropout(0.2) (c4)\n",
        "  c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "  p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "  c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "  c5 = Dropout(0.3) (c5)\n",
        "  c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "  u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "  u6 = concatenate()([u6, c4])\n",
        "  c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "  c6 = Dropout(0.2) (c6)\n",
        "  c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "  u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "  u7 = concatenate()([u7, c3])\n",
        "  c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "  c7 = Dropout(0.2) (c7)\n",
        "  c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "  u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "  u8 = concatenate()([u8, c2])\n",
        "  c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "  c8 = Dropout(0.1) (c8)\n",
        "  c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "\n",
        "  u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "  u9 = concatenate(axis=3)([u9, c1])\n",
        "  c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "  c9 = Dropout(0.1) (c9)\n",
        "  c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "\n",
        "  outputs = Conv2D(2, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "  # Model compilation\n",
        "  model.compile(optimizer='adam', loss = loss, metrics = metrics)\n",
        "\n",
        "  # Return model\n",
        "  return model"
      ],
      "metadata": {
        "id": "zuy0TB9gxjax"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Net input generator\n",
        "train_generator_augmented = DataGenerator().train_generator_augmented(batch_size)\n",
        "val_generator = DataGenerator().val_generator(batch_size)"
      ],
      "metadata": {
        "id": "uUyeVf7pcu2T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model trained with Dice Loss - Augmentation"
      ],
      "metadata": {
        "id": "ONMUxf9NEioD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = create_model(loss)\n",
        "\n",
        "# Fit model\n",
        "model_history = model.fit(x = train_generator_augmented, \n",
        "                          epochs = epochs, \n",
        "                          steps_per_epoch=(len(train_set)*2//batch_size),\n",
        "                          validation_data = val_generator,\n",
        "                          validation_steps = (len(val_set)//batch_size),\n",
        "                          callbacks = callback, \n",
        "                          )"
      ],
      "metadata": {
        "id": "rPzCzuBF1ITa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4567f861-ae54-493f-c563-b52eacce6d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 86/328 [======>.......................] - ETA: 59:45 - loss: 0.0261 - dice_coef: 0.9739 - true_positive_rate: 0.9920 - IoU: 0.9567  "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights\n",
        "model.save_weights(f'./trained_models/dice_loss_aug_model/dice_loss_aug')"
      ],
      "metadata": {
        "id": "wXGdC729BxDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model instance\n",
        "new_model = create_model(loss)\n",
        "\n",
        "# Restore the weights\n",
        "new_model.load_weights(f'./trained_models/dice_loss_aug_model/dice_loss_aug')"
      ],
      "metadata": {
        "id": "zIUtoXtzBv0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(model_history.history).to_csv(f'./trained_models_history/dice_loss_aug')"
      ],
      "metadata": {
        "id": "2aCnV2XFpcOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  #pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return np.array(pred_mask[0], dtype = 'float32')"
      ],
      "metadata": {
        "id": "pxnQaOq7QbLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Prediction Example"
      ],
      "metadata": {
        "id": "ra6jlAlntX0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "loss_3 = dice_loss\n",
        "\n",
        "# Create model\n",
        "focal_model = create_model(loss_3)\n",
        "\n",
        "# Restore the weights\n",
        "focal_model.load_weights(f'./trained_models/dice_loss_aug_model/dice_loss_aug')\n",
        "\n",
        "history_focal = pd.read_csv('./trained_models_history/dice_loss')"
      ],
      "metadata": {
        "id": "5-wlKwuTUyNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_pixels = pd.read_csv(\"encoded_pixels.csv\")"
      ],
      "metadata": {
        "id": "OEuaTkPg62p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patient\n",
        "idx = \"1.2.276.0.7230010.3.1.4.8323329.32579.1517875161.299312\"\n",
        "\n",
        "# Image\n",
        "dcmHead = dcmread(\"./dicom_files/\" + idx + \".dcm\")\n",
        "true_img = dcmHead.pixel_array / 255\n",
        "true_img = np.expand_dims(true_img, axis=-1)\n",
        "true_img = tf.keras.layers.Resizing(256, 256, interpolation=\"bilinear\", crop_to_aspect_ratio=False)(true_img)\n",
        "\n",
        "# True mask\n",
        "true_mask = encoded_pixels.iloc[4][\" EncodedPixels\"]\n",
        "true_mask = rle2mask(true_mask, 1024, 1024)\n",
        "true_mask = np.expand_dims(true_mask, axis=-1)\n",
        "true_mask = tf.keras.layers.Resizing(256, 256, interpolation=\"nearest\", crop_to_aspect_ratio=False)(true_mask)"
      ],
      "metadata": {
        "id": "_8DyCZGq56dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# True\n",
        "plt.subplot(121)\n",
        "img = np.array(true_img[:150, :150, 0])\n",
        "mask = np.array(true_mask[:150, :150, 0])\n",
        "#mask[mask<0.5]=np.nan\n",
        "\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.imshow(mask, alpha=0.4,cmap='Reds',vmin=0,vmax=1)\n",
        "plt.axis('off')\n",
        "\n",
        "# Predicted\n",
        "plt.subplot(122)\n",
        "pred_mask = focal_model.predict(np.expand_dims(true_img, axis=0))\n",
        "pred_mask = create_mask(pred_mask)\n",
        "# pred_mask[pred_mask<0.5] = np.nan\n",
        "\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.imshow(pred_mask[:150, :150], alpha=0.4,cmap='Reds', vmin=0, vmax=1)\n",
        "plt.axis('off')\n"
      ],
      "metadata": {
        "id": "p7cnKD047Da0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}