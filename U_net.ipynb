{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHOpwq13hZ5i/ieZydeesW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niccolo-Rocchi/Medical_Imaging_project/blob/main/U_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "UAzOXcdIz2JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!pip install pydicom"
      ],
      "metadata": {
        "id": "Y9Wl_73Mz8XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVqMeQ_GxbHh"
      },
      "outputs": [],
      "source": [
        "# For reading files\n",
        "from pydicom import dcmread \n",
        "import glob\n",
        "from google.colab import drive\n",
        "# For dealing with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "random.seed(123)\n",
        "# For plots\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread, imshow\n",
        "# For nets\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers import Concatenate as concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive data\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/pneumotorax_data"
      ],
      "metadata": {
        "id": "yg9w3Cm-01pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data upload"
      ],
      "metadata": {
        "id": "7xEjXYjqN7lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read patients IDs and their encoded pixels\n",
        "encoded_pixels = pd.read_csv('./encoded_pixels.csv')\n",
        "# Read dicom files IDs\n",
        "dicom_files_IDs = glob.glob('./dicom_files/*')\n",
        "dicom_files_IDs = pd.DataFrame({\"dicom_ID\" : [re.findall(r\"\\./dicom_files/(.+)\\.dcm\", id)[0] for id in dicom_files_IDs]})\n",
        "# Intersect patients IDs with dicom IDs\n",
        "encoded_pixels = pd.merge(encoded_pixels, dicom_files_IDs, how = \"inner\", left_on = \"ImageId\", right_on = \"dicom_ID\")\n",
        "# Mark healthy patients as \"0\"\n",
        "encoded_pixels[\" EncodedPixels\"] = encoded_pixels[\" EncodedPixels\"].apply(lambda x: x.split())\n",
        "encoded_pixels[\"Health\"] = encoded_pixels[\" EncodedPixels\"].apply(lambda x: 0 if len(x)==1 else 1)"
      ],
      "metadata": {
        "id": "30s0N7ObVVOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find by how many pts healthy vs unhealthy the dataset differ\n",
        "diff = len(encoded_pixels[encoded_pixels[\"Health\"] == 0]) - len(encoded_pixels[encoded_pixels[\"Health\"] == 1])\n",
        "# Select this number of pts in a random manner and drop them in the dataset\n",
        "healthy_idx = list(encoded_pixels[encoded_pixels[\"Health\"] == 0].index)\n",
        "random.shuffle(healthy_idx)\n",
        "encoded_pixels.drop(index = healthy_idx[:diff], inplace = True)\n",
        "# Assert\n",
        "assert(len(encoded_pixels[encoded_pixels[\"Health\"] == 0]) == len(encoded_pixels[encoded_pixels[\"Health\"] == 1]))"
      ],
      "metadata": {
        "id": "mZfFUHgATHAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset indices\n",
        "encoded_pixels.reset_index(drop = True, inplace = True)\n",
        "# Sample the dataset\n",
        "train_frac = 0.8\n",
        "train_set = encoded_pixels.sample(frac = train_frac, random_state = 123)\n",
        "val_set = encoded_pixels.drop(index = train_set.index).sample(frac = 1, random_state = 1)\n",
        "# Assert there's no intersection between two sets\n",
        "assert([i for i in val_set.index if i in train_set.index] == [])\n",
        "# Make size compatible with batches\n",
        "max_batch_size = 32\n",
        "train_set = train_set.iloc[:-(len(train_set)%max_batch_size)]\n",
        "val_set = val_set.iloc[:-(len(val_set)%max_batch_size)]\n",
        "# Reset indices\n",
        "train_set.reset_index(drop = True, inplace = True)\n",
        "val_set.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "U7i2-Xn2eEpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RLE to mask conversion provided by competition organizers with the dataset.\n",
        "def rle2mask(rle, width, height):\n",
        "    mask= np.zeros(width* height)\n",
        "    array = np.asarray([int(x) for x in rle])\n",
        "    starts = array[0::2]\n",
        "    lengths = array[1::2]\n",
        "\n",
        "    current_position = 0\n",
        "    if rle == ['-1']:\n",
        "      return mask.reshape(width, height, order='F')\n",
        "    else:\n",
        "      for index, start in enumerate(starts):\n",
        "          current_position += start\n",
        "          mask[current_position:current_position+lengths[index]] = 255\n",
        "          current_position += lengths[index]\n",
        "      return mask.reshape(width, height, order='F')"
      ],
      "metadata": {
        "id": "4L1i9ZtCEyRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input for Keras' fit function\n",
        "class DataGenerator:\n",
        "\n",
        "  # Method that yields (image, mask) tuple\n",
        "  def data_generator(self, data):\n",
        "    i = 0\n",
        "    while True:\n",
        "      while i < len(data):\n",
        "        # Extract ID and its encoded pixels\n",
        "        id, rle = data[[\"ImageId\",\" EncodedPixels\"]].iloc[i]\n",
        "        # Convert encoded pixels to mask\n",
        "        mask = rle2mask(rle, 1024, 1024)\n",
        "        # Read the image associate to ImageId\n",
        "        try:\n",
        "          dcm_file = dcmread(f\"./dicom_files/{id}.dcm\")\n",
        "        except:\n",
        "          continue\n",
        "        dcm_image = dcm_file.pixel_array\n",
        "        # Rescale image\n",
        "        image = dcm_image/255\n",
        "        # Expand dimensions\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        # Resize image and mask\n",
        "        mask = tf.keras.layers.Resizing(256, 256, interpolation=\"nearest\", crop_to_aspect_ratio=False)(mask)\n",
        "        image = tf.keras.layers.Resizing(256, 256, interpolation=\"bilinear\", crop_to_aspect_ratio=False)(image)\n",
        "        yield (image, mask)\n",
        "        i += 1\n",
        "\n",
        "  # Create train generator for net input\n",
        "  def train_generator(self, batch_size):\n",
        "    # Create a tensorflow iterator\n",
        "    tf_iterator = tf.data.Dataset.from_generator(lambda: self.data_generator(train_set), output_types=(tf.float64, tf.float64))\n",
        "    # Create batch size\n",
        "    tf_iterator = tf_iterator.batch(batch_size)\n",
        "    # Return generator\n",
        "    return tf_iterator\n",
        "\n",
        "  # Create validation generator for net input\n",
        "  def val_generator(self, batch_size):\n",
        "    # Create a tensorflow iterator\n",
        "    tf_iterator = tf.data.Dataset.from_generator(lambda: self.data_generator(val_set), output_types=(tf.float64, tf.float64))\n",
        "    # Create batch size\n",
        "    tf_iterator = tf_iterator.batch(batch_size)\n",
        "    # Return generator\n",
        "    return tf_iterator"
      ],
      "metadata": {
        "id": "qBhUUfwm18_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. https://towardsdatascience.com/medical-image-dataloaders-in-tensorflow-2-x-ee5327a4398f\n",
        "2. https://stackoverflow.com/questions/55375416/tensorflow-model-fit-using-a-dataset-generator\n",
        "3. https://faroit.com/keras-docs/1.2.0/models/model/\n",
        "4. https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling\n",
        "5. https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#from_generator"
      ],
      "metadata": {
        "id": "X2qpVhbBN-HC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-net"
      ],
      "metadata": {
        "id": "dBmx8nY4hciT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and other metrics\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
        "\n",
        "def dice_p_bce(in_gt, in_pred):\n",
        "    \"\"\"combine DICE and BCE\"\"\"\n",
        "    return 0.01*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)"
      ],
      "metadata": {
        "id": "FqMreoG0e3UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Net definition\n",
        "inputs = Input((256, 256, 1))\n",
        "\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\n",
        "c1 = Dropout(0.1) (c1)\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "c2 = Dropout(0.1) (c2)\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "c3 = Dropout(0.2) (c3)\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "c4 = Dropout(0.2) (c4)\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "c5 = Dropout(0.3) (c5)\n",
        "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "u6 = concatenate()([u6, c4])\n",
        "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "c6 = Dropout(0.2) (c6)\n",
        "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "u7 = concatenate()([u7, c3])\n",
        "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "c7 = Dropout(0.2) (c7)\n",
        "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "u8 = concatenate()([u8, c2])\n",
        "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "c8 = Dropout(0.1) (c8)\n",
        "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "\n",
        "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "u9 = concatenate(axis=3)([u9, c1])\n",
        "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "c9 = Dropout(0.1) (c9)\n",
        "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "\n",
        "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)"
      ],
      "metadata": {
        "id": "x6dYJWZgmifm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(gamma=2., alpha=.25):\n",
        "\tdef focal_loss_fixed(y_true, y_pred):\n",
        "\t\tpt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "\t\tpt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\t\treturn -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1+K.epsilon())) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
        "\treturn focal_loss_fixed"
      ],
      "metadata": {
        "id": "MvEIhO6L8VOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "model = Model(inputs=[inputs], outputs=[outputs])\n",
        "# Model compilation\n",
        "model.compile(optimizer='adam', loss=focal_loss(), metrics=[dice_p_bce])\n",
        "# Model summary\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "IZvv3b9LiGDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch and epochs definition\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "assert(batch_size <= max_batch_size)"
      ],
      "metadata": {
        "id": "WedC82uejOSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Net input generator initialization\n",
        "train_generator = DataGenerator().train_generator(batch_size)\n",
        "val_generator = DataGenerator().val_generator(batch_size)"
      ],
      "metadata": {
        "id": "f4l0E--gig39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assert generators are not empty (run the cell just for check, not prior to net fit)\n",
        "# _exhausted  = object()\n",
        "# assert(next(train_generator, _exhausted) is not _exhausted)\n",
        "# assert(next(val_generator, _exhausted) is not _exhausted)"
      ],
      "metadata": {
        "id": "8KE9WV2xoU5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3) "
      ],
      "metadata": {
        "id": "0T9jkjynmk60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fit\n",
        "model_history = model.fit(x = train_generator, \n",
        "                          epochs = epochs, \n",
        "                          steps_per_epoch=(len(train_set)//batch_size),\n",
        "                          validation_data = val_generator,\n",
        "                          validation_steps = (len(val_set)//batch_size - 1),\n",
        "                          validation_batch_size = batch_size,\n",
        "                          callbacks = callback, \n",
        "                          )"
      ],
      "metadata": {
        "id": "t_dqMxZaiL7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot losses\n",
        "train_loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.plot(val_loss)"
      ],
      "metadata": {
        "id": "X1OkFRxoWmS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history.history"
      ],
      "metadata": {
        "id": "sjHa1hNGIlO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}